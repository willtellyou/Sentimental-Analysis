{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Necessary Libraries for implementation**"
      ],
      "metadata": {
        "id": "HPJ0atEcsMQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y32CB3Sbl6DD"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "from future.utils import iteritems\n",
        "from builtins import range"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "y2pvWwjbncoy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "N9I6Y3kvnhO6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "maROfY5dnk6z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting a list of stopwords**\n",
        "\n",
        "Link for downloading file: http://www.lextek.com/manuals/onix/stopwords1.html "
      ],
      "metadata": {
        "id": "JnPcg6Phs7n6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "an alternative source of stopwords\n",
        " #from nltk.corpus import stopwords\n",
        " #stopwords.words('english')"
      ],
      "metadata": {
        "id": "7qk3EeQQtZDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(w.rstrip() for w in open('stopwords.txt'))"
      ],
      "metadata": {
        "id": "mDL8D2jsnweJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load the reviews\n",
        "\n",
        " data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html"
      ],
      "metadata": {
        "id": "pmulXPO2tuRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_reviews = BeautifulSoup(open('positive.review.txt').read(), features=\"lxml\")\n",
        "positive_reviews = positive_reviews.findAll('review_text')"
      ],
      "metadata": {
        "id": "K6gek4EPn5jC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_reviews = BeautifulSoup(open('negative.review.txt').read(), features=\"lxml\")\n",
        "negative_reviews = negative_reviews.findAll('review_text')"
      ],
      "metadata": {
        "id": "nOL6eZ7moEnG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "first let's just try to tokenize the text using nltk's tokenizer\n",
        "\n",
        "let's take the first review for example:\n",
        "\n",
        "[t = positive_reviews[0]\n",
        "\n",
        "nltk.tokenize.word_tokenize(t.text)]\n",
        "\n",
        "notice how it doesn't downcase, so It != it\n",
        "\n",
        "not only that, but do we really want to include the word \"it\" anyway?\n",
        "\n",
        "you can imagine it wouldn't be any more common in a positive review than a \n",
        "negative review\n",
        "\n",
        "so it might only add noise to our model.\n",
        "\n",
        "so let's create a function that does all this pre-processing for us"
      ],
      "metadata": {
        "id": "RhxvxG2fuDPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_tokenizer(s):\n",
        "    s = s.lower() # downcase\n",
        "    tokens = nltk.tokenize.word_tokenize(s) # split string into words (tokens)\n",
        "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
        "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
        "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "JgA9veDZoJCz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a word-to-index map so that we can create our word-frequency vectors later\n",
        "\n",
        " let's also save the tokenized versions so we don't have to tokenize again later"
      ],
      "metadata": {
        "id": "aDqFDDbbupqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index_map = {}\n",
        "current_index = 0\n",
        "positive_tokenized = []\n",
        "negative_tokenized = []\n",
        "orig_reviews = []"
      ],
      "metadata": {
        "id": "cne6w_MKoNxi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading the necessary libraries from nltk**"
      ],
      "metadata": {
        "id": "SlnNNY_nutNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUI4ZeFTpkaY",
        "outputId": "6eb11ebf-580d-4887-8d7c-c422101a4677"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH2f5pzWptmk",
        "outputId": "9a68e192-e196-407c-eeae-b5e60213fc7d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5F66deDp0bP",
        "outputId": "b51d0680-8498-4840-cbc6-0c50f4a48f52"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for review in positive_reviews:\n",
        "    orig_reviews.append(review.text)\n",
        "    tokens = my_tokenizer(review.text)\n",
        "    positive_tokenized.append(tokens)\n",
        "    for token in tokens:\n",
        "        if token not in word_index_map:\n",
        "            word_index_map[token] = current_index\n",
        "            current_index += 1"
      ],
      "metadata": {
        "id": "kBWhevl4oSGc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for review in negative_reviews:\n",
        "    orig_reviews.append(review.text)\n",
        "    tokens = my_tokenizer(review.text)\n",
        "    negative_tokenized.append(tokens)\n",
        "    for token in tokens:\n",
        "        if token not in word_index_map:\n",
        "            word_index_map[token] = current_index\n",
        "            current_index += 1"
      ],
      "metadata": {
        "id": "jcREctMzp5z2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"len(word_index_map):\", len(word_index_map))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5VZODw0qCPI",
        "outputId": "8dbe2092-386f-4695-b6e6-82d6d834760f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(word_index_map): 10948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now let's create our input matrices"
      ],
      "metadata": {
        "id": "ddYSKH8pvFEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_vector(tokens, label):\n",
        "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
        "    for t in tokens:\n",
        "        i = word_index_map[t]\n",
        "        x[i] += 1\n",
        "    x = x / x.sum() # normalize it before setting label\n",
        "    x[-1] = label\n",
        "    return x"
      ],
      "metadata": {
        "id": "tQtvoMe8qG9U"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(positive_tokenized) + len(negative_tokenized)\n",
        "# (N x D+1 matrix - keeping them together for now so we can shuffle more easily later\n",
        "data = np.zeros((N, len(word_index_map) + 1))\n",
        "i = 0"
      ],
      "metadata": {
        "id": "aCb8HPgnqLC9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tokens in positive_tokenized:\n",
        "    xy = tokens_to_vector(tokens, 1)\n",
        "    data[i,:] = xy\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "78QoqIUHqPkL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tokens in negative_tokenized:\n",
        "    xy = tokens_to_vector(tokens, 0)\n",
        "    data[i,:] = xy\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "OwtDRvX2qV7t"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "shuffle the data and create train/test splits"
      ],
      "metadata": {
        "id": "NEjw1d8pvKbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orig_reviews, data = shuffle(orig_reviews, data)"
      ],
      "metadata": {
        "id": "6HDSNCfYqXdN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[:,:-1]\n",
        "Y = data[:,-1]"
      ],
      "metadata": {
        "id": "RLlblHnQrkIL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "last 100 rows will be test"
      ],
      "metadata": {
        "id": "UMID3Q0BvOoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain = X[:-100,]\n",
        "Ytrain = Y[:-100,]\n",
        "Xtest = X[-100:,]\n",
        "Ytest = Y[-100:,]"
      ],
      "metadata": {
        "id": "Ei6x920jrorS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(Xtrain, Ytrain)\n",
        "print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
        "print(\"Test accuracy:\", model.score(Xtest, Ytest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk77CcjRrsZc",
        "outputId": "aef09b85-f7fc-4382-a1be-beef6c2205f4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.7842105263157895\n",
            "Test accuracy: 0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's look at the weights for each word"
      ],
      "metadata": {
        "id": "xflTULxOvUYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5\n",
        "for word, index in iteritems(word_index_map):\n",
        "    weight = model.coef_[0][index]\n",
        "    if weight > threshold or weight < -threshold:\n",
        "        print(word, weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H9yMCAerz1Q",
        "outputId": "318bf469-5453-4edb-f722-24a1ff9cc9a0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unit -0.737895751518615\n",
            "bad -0.7928197937863067\n",
            "cable 0.5659943949932409\n",
            "time -0.5977058923416557\n",
            "'ve 0.74880187153881\n",
            "month -0.7834323741158365\n",
            "sound 0.9840963701879415\n",
            "lot 0.586952295889688\n",
            "you 1.0458997746532022\n",
            "n't -2.083028657548949\n",
            "easy 1.7709332688917743\n",
            "quality 1.4060061624571984\n",
            "company -0.5669853583815873\n",
            "item -0.9845688251717922\n",
            "wa -1.6159306611491042\n",
            "perfect 0.9959313767105659\n",
            "fast 0.9709390116417677\n",
            "ha 0.6836983691616917\n",
            "price 2.6306778318226174\n",
            "value 0.5434826838501908\n",
            "money -1.111246846068905\n",
            "memory 0.985920738810667\n",
            "buy -0.8837188239896172\n",
            "bit 0.6027602644776905\n",
            "happy 0.6483995874911666\n",
            "pretty 0.730993791979793\n",
            "doe -1.2000788972859866\n",
            "highly 1.075781379771506\n",
            "recommend 0.6417621638154619\n",
            "customer -0.6386738828366063\n",
            "support -0.8081001275550859\n",
            "little 0.9184422057734457\n",
            "returned -0.7855152271696372\n",
            "excellent 1.2843440296194788\n",
            "love 1.2527278939795603\n",
            "piece -0.5683585045433528\n",
            "useless -0.5234679625005486\n",
            "week -0.7789700542175677\n",
            "using 0.6485848920994011\n",
            "laptop 0.5609882573575738\n",
            "video 0.5787513709261247\n",
            "poor -0.7603782435703313\n",
            "then -1.1216628559231792\n",
            "tried -0.7803466087160469\n",
            "try -0.5833830174087979\n",
            "space 0.6279723033198918\n",
            "comfortable 0.6479984648609514\n",
            "hour -0.5588339971311826\n",
            "speaker 0.6655078654841958\n",
            "warranty -0.5895625997362482\n",
            "stopped -0.5474834399663651\n",
            "junk -0.5564893109786712\n",
            "returning -0.5437262305733216\n",
            "paper 0.6194900881754118\n",
            "terrible -0.5106931258964122\n",
            "return -1.1940773624530903\n",
            "waste -1.004198643058438\n",
            "refund -0.61291301928786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check misclassified examples"
      ],
      "metadata": {
        "id": "6eG8FHo5vbgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(X)\n",
        "P = model.predict_proba(X)[:,1] # p(y = 1 | x)"
      ],
      "metadata": {
        "id": "maYwSDlyr3Zl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "since there are many, just print the \"most\" wrong samples"
      ],
      "metadata": {
        "id": "yQT2CwgZvhBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "minP_whenYis1 = 1\n",
        "maxP_whenYis0 = 0\n",
        "wrong_positive_review = None\n",
        "wrong_negative_review = None\n",
        "wrong_positive_prediction = None\n",
        "wrong_negative_prediction = None"
      ],
      "metadata": {
        "id": "SSQOn7Jfr98H"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(N):\n",
        "    p = P[i]\n",
        "    y = Y[i]\n",
        "    if y == 1 and p < 0.5:\n",
        "        if p < minP_whenYis1:\n",
        "            wrong_positive_review = orig_reviews[i]\n",
        "            wrong_positive_prediction = preds[i]\n",
        "            minP_whenYis1 = p\n",
        "    elif y == 0 and p > 0.5:\n",
        "        if p > maxP_whenYis0:\n",
        "            wrong_negative_review = orig_reviews[i]\n",
        "            wrong_negative_prediction = preds[i]\n",
        "            maxP_whenYis0 = p"
      ],
      "metadata": {
        "id": "owx4Io4jr_dX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most wrong positive review (prob = %s, pred = %s):\" % (minP_whenYis1, wrong_positive_prediction))\n",
        "print(wrong_positive_review)\n",
        "print(\"Most wrong negative review (prob = %s, pred = %s):\" % (maxP_whenYis0, wrong_negative_prediction))\n",
        "print(wrong_negative_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUtHA9ezsDD6",
        "outputId": "1025abed-78e1-4382-c374-8cb10d688e9e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most wrong positive review (prob = 0.35392278011586775, pred = 0.0):\n",
            "\n",
            "A device like this either works or it doesn't.  This one happens to work\n",
            "\n",
            "Most wrong negative review (prob = 0.6021959922952513, pred = 1.0):\n",
            "\n",
            "The Voice recorder meets all my expectations and more\n",
            "Easy to use, easy to transfer great results\n",
            "\n"
          ]
        }
      ]
    }
  ]
}